{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sps\n",
    "from scipy.optimize import minimize\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timing(f):\n",
    "    def wrap(*args):\n",
    "        time1 = time.time()\n",
    "        ret = f(*args)\n",
    "        time2 = time.time()\n",
    "        print('{:s} function took {:.3f} ms'.format(f.__name__, (time2-time1)*1000.0))\n",
    "\n",
    "        return ret\n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_proj_simplex(v, s=1):\n",
    "    n, = v.shape  # will raise ValueError if v is not 1-D\n",
    "    # check if we are already on the simplex\n",
    "    if v.sum() == s and np.alltrue(v >= 0):\n",
    "        # best projection: itself!\n",
    "        return v\n",
    "    # get the array of cumulative sums of a sorted (decreasing) copy of v\n",
    "    u = np.sort(v)[::-1]\n",
    "    cssv = np.cumsum(u)\n",
    "    # get the number of > 0 components of the optimal solution\n",
    "    rho = np.nonzero(u * np.arange(1, n+1) > (cssv - s))[0][-1]\n",
    "    # compute the Lagrange multiplier associated to the simplex constraint\n",
    "    theta = (cssv[rho] - s) / (rho + 1.0)\n",
    "    # compute the projection by thresholding v using theta\n",
    "    w = (v - theta).clip(min=0)\n",
    "    return w\n",
    "\n",
    "\n",
    "def ternary_search(func, left, right, precision):\n",
    "    # finds argmax of convex function f\n",
    "    # on the segment [left; right] with given precision\n",
    "    if abs(right - left) < precision:\n",
    "        return left\n",
    "\n",
    "    left_third = (2*left + right)/3\n",
    "    right_third = (left + 2*right)/3\n",
    "    if func(left_third) == func(right_third):\n",
    "        return ternary_search(func, left_third, right_third, precision) \n",
    "    if func(left_third) < func(right_third):\n",
    "        return ternary_search(func, left_third, right, precision) \n",
    "    else:\n",
    "        return ternary_search(func, left, right_third, precision)\n",
    "\n",
    "\n",
    "def line_search(f, precision=1e-7, estimate=0):\n",
    "    # finds argmax of convex function f\n",
    "    # on the line with given precision\n",
    "    curr = estimate\n",
    "    delta = 1\n",
    "    while f(curr + delta) > f(curr):\n",
    "        curr = curr + delta\n",
    "        delta *= 2\n",
    "    curr_left = estimate\n",
    "    delta_left = 1\n",
    "    while f(curr_left - delta_left) > f(curr_left):\n",
    "        curr_left = curr_left - delta_left\n",
    "        delta_left *= 2\n",
    "    return ternary_search(f, curr_left - delta_left, curr + delta, precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent method\n",
    "@timing\n",
    "def proj_grad_descent(x0, precision, max_iters):\n",
    "    cur_x = x0\n",
    "    previous_step_size = 1\n",
    "    iters = 0\n",
    "    while previous_step_size > precision and iters < max_iters:\n",
    "        prev_x = cur_x\n",
    "        grad = df(prev_x)\n",
    "        \n",
    "        def func(alpha):\n",
    "            return f(euclidean_proj_simplex(prev_x + alpha * grad))\n",
    "        alpha = line_search(func)\n",
    "\n",
    "        cur_x = euclidean_proj_simplex(prev_x + alpha * grad)\n",
    "        previous_step_size = np.linalg.norm(cur_x - prev_x)\n",
    "        iters += 1\n",
    "        \n",
    "    np.set_printoptions(precision=4)\n",
    "    print(\"The local maximum of proj grad occurs at\", cur_x)\n",
    "    print(\"Number of gradient iterations: \", iters)\n",
    "    return cur_x\n",
    "\n",
    "\n",
    "# Quasi-Newton method\n",
    "@timing\n",
    "def proj_quasi_newton_method (x0, precision, max_iters):\n",
    "    # DFP algorithm\n",
    "    cur_x = x0\n",
    "    previous_step_size = 1\n",
    "    iters = 0\n",
    "    H = np.eye(np.size(x0))\n",
    "    alpha = 1\n",
    "    while previous_step_size > precision and iters < max_iters:\n",
    "        prev_x = cur_x\n",
    "        h = -H @ df(prev_x)\n",
    "        \n",
    "        def func(alpha):\n",
    "            return f(euclidean_proj_simplex(prev_x + alpha * h))\n",
    "        \n",
    "        alpha = line_search(func)\n",
    "        \n",
    "        cur_x = euclidean_proj_simplex(prev_x + alpha * h)\n",
    "        s = np.atleast_2d(cur_x - prev_x).T\n",
    "        y = np.atleast_2d(df(cur_x) - df(prev_x)).T\n",
    "        \n",
    "        H = H - (H @ y @ y.T @ H) / (y.T @ H @ y) + (s @ s.T) / (y.T @ s)\n",
    "        previous_step_size = np.linalg.norm(cur_x - prev_x) \n",
    "        iters += 1\n",
    "        \n",
    "    np.set_printoptions(precision=4)\n",
    "    print(\"The local maximum of quasi newton occurs at\", cur_x)\n",
    "    print(\"Number of quasi-newton iterations: \", iters)\n",
    "    return cur_x\n",
    "\n",
    "\n",
    "# Newton method \n",
    "@timing\n",
    "def proj_newton_method (x0, precision, max_iters):\n",
    "    cur_x = x0\n",
    "    previous_step_size = 1\n",
    "    iters = 0\n",
    "    while previous_step_size > precision and iters < max_iters:\n",
    "        prev_x = cur_x\n",
    "        h = - np.linalg.inv(ddf(prev_x)) @ df(prev_x)\n",
    "        def func(alpha):\n",
    "            return f(euclidean_proj_simplex(prev_x + alpha * h))\n",
    "        \n",
    "        alpha = line_search(func)\n",
    "        cur_x = euclidean_proj_simplex(prev_x + alpha * h)\n",
    "        previous_step_size = np.linalg.norm(cur_x - prev_x) \n",
    "        iters += 1\n",
    "        \n",
    "    np.set_printoptions(precision=4)\n",
    "    print(\"The local maximum of newton occurs at\", cur_x)\n",
    "    print(\"Number of newton iterations: \", iters)\n",
    "    return cur_x\n",
    "\n",
    "\n",
    "# Interior point method\n",
    "@timing\n",
    "def interior_point_method(x0, t0, alpha, precision, max_iters):   \n",
    "    n = x0.size\n",
    "    A = np.atleast_2d(np.ones(n))\n",
    "    b = np.ones(1)\n",
    "    cur_x = x0\n",
    "    t = t0\n",
    "    \n",
    "    def phi(x, t):\n",
    "        if np.min(x) <= 0:\n",
    "            return -np.inf\n",
    "        return f(x) + t * np.sum(np.log(x))\n",
    "    \n",
    "    def phi_grad(x, t):\n",
    "        return df(x) + t/x\n",
    "    \n",
    "    def phi_hess(x, t):\n",
    "        return ddf(x) - t * np.diag(1/(x**2))\n",
    "    \n",
    "    def newton_equality_feasible(x0, A, b, t):\n",
    "        # solves maximization problem for phi(x, t) with constraints Ax = b \n",
    "        cur_x = x0\n",
    "        m = A.shape[0]\n",
    "        n = A.shape[1]\n",
    "        while True:\n",
    "            newton_matrix = np.bmat([[-phi_hess(cur_x, t), A.T], [A, np.zeros((m, m))]])\n",
    "            rhs = np.atleast_2d(np.hstack([phi_grad(cur_x, t), np.zeros(m)])).T\n",
    "            w = np.linalg.solve(newton_matrix, rhs)\n",
    "            h = w[:n]\n",
    "            if np.abs(h.T @ phi_hess(cur_x, t) @ h) < precision:\n",
    "                break\n",
    "            \n",
    "            h = h.reshape(n)\n",
    "            \n",
    "            def func(alpha):\n",
    "                return phi(cur_x + alpha * h, t)\n",
    "            \n",
    "            alpha = line_search(func)\n",
    "            cur_x = cur_x + alpha * h\n",
    "        return cur_x\n",
    "   \n",
    "    while True:\n",
    "        cur_x = newton_equality_feasible(cur_x, A, b, t)\n",
    "        if n * t < precision:\n",
    "            break\n",
    "        t *= alpha\n",
    "    print(\"The local maximum of interior point method occurs at\", cur_x)\n",
    "    return cur_x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "n = 15\n",
    "m = 15\n",
    "\n",
    "P = np.random.rand(n, m) \n",
    "P /= P.sum(axis=1)[:,None]\n",
    "P = P.T\n",
    "c = (P * np.log2(P)).sum(axis=0)\n",
    "\n",
    "def f(x):\n",
    "    array = (P @ x) * np.log2(P @ x)\n",
    "    return c @ x - array.sum() \n",
    "\n",
    "def df(x):\n",
    "    k = P.shape[0]\n",
    "    array = np.array([P[i] * (np.log2(P[i] @ x) + 1/np.log(2)) for i in range(k)])\n",
    "    return c - array.sum(axis=0)\n",
    "\n",
    "def ddf(x):\n",
    "    k = P.shape[0]\n",
    "    t = P @ x\n",
    "    array = np.array([np.atleast_2d(P[i]).T @ np.atleast_2d(P[i]) for i in range(k)])\n",
    "    result = - np.array([array[i] / (np.log(2) * t[i]) for i in range(k)]).sum(axis=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting point: [0.115  0.1288 0.0214 0.003  0.0074 0.0263 0.128  0.0642 0.0021 0.1273\n",
      " 0.0658 0.0699 0.0845 0.0063 0.15  ]\n",
      "The local maximum of proj grad occurs at [0.2837 0.     0.     0.     0.3022 0.     0.0823 0.     0.0343 0.2975\n",
      " 0.     0.     0.     0.     0.    ]\n",
      "Number of gradient iterations:  17\n",
      "proj_grad_descent function took 173.123 ms\n",
      "max proj grad capacity: 0.38635614119617356 \n",
      "\n",
      "The local maximum of quasi newton occurs at [3.1476e-01 4.0510e-02 6.9602e-03 1.2709e-03 2.4308e-01 4.7871e-03\n",
      " 1.3728e-01 2.6195e-02 2.5217e-02 1.4944e-01 3.9882e-09 4.2166e-03\n",
      " 5.2687e-03 2.4372e-03 3.8579e-02]\n",
      "Number of quasi-newton iterations:  5\n",
      "proj_quasi_newton_method function took 57.561 ms\n",
      "max proj quasi newton capacity: 0.36931540933771734 \n",
      "\n",
      "The local maximum of newton occurs at [1.0253e-01 9.1133e-02 2.5834e-02 4.7385e-06 2.4784e-02 3.4228e-02\n",
      " 1.1171e-01 6.6373e-02 2.9653e-07 1.3475e-01 4.8772e-02 8.2141e-02\n",
      " 8.3655e-02 3.3671e-02 1.6042e-01]\n",
      "Number of newton iterations:  1000\n",
      "proj_newton_method function took 13638.987 ms\n",
      "max proj newton capacity: 0.2843988881006112 \n",
      "\n",
      "The local maximum of interior point method occurs at [2.8367e-01 3.3968e-07 1.2255e-06 1.9161e-07 3.0220e-01 5.3633e-07\n",
      " 8.2307e-02 6.5027e-05 3.4353e-02 2.9740e-01 2.2243e-07 2.4552e-07\n",
      " 6.4306e-08 3.9958e-08 3.0621e-07]\n",
      "interior_point_method function took 222.268 ms\n",
      "max interior point method capacity: 0.38635589813174764\n"
     ]
    }
   ],
   "source": [
    "x0 = np.random.rand(n)\n",
    "x0 /= x0.sum()\n",
    "precision = 10**-6\n",
    "iters = 10**3\n",
    "\n",
    "print('Starting point:', x0)\n",
    "\n",
    "x1 = proj_grad_descent(x0, precision, iters)\n",
    "print(\"max proj grad capacity:\", f(x1), '\\n')\n",
    "x2 = proj_quasi_newton_method (x0,  precision, iters)\n",
    "print(\"max proj quasi newton capacity:\", f(x2), '\\n')\n",
    "x3 = proj_newton_method (x0, precision, iters)\n",
    "print(\"max proj newton capacity:\", f(x3), '\\n')\n",
    "x4 = interior_point_method(x0, 1, 0.1, precision, iters)\n",
    "print(\"max interior point method capacity:\", f(x4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in log2\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "     fun: -0.3863496511409541\n",
       "     jac: array([1.0564, 1.1256, 1.0769, 1.2275, 1.0574, 1.0679, 1.0564, 1.0566,\n",
       "       1.0579, 1.055 , 1.2164, 1.1402, 1.1464, 1.2059, 1.1734])\n",
       " message: 'Iteration limit exceeded'\n",
       "    nfev: 1952\n",
       "     nit: 101\n",
       "    njev: 101\n",
       "  status: 9\n",
       " success: False\n",
       "       x: array([ 2.8338e-01, -3.5128e-06, -3.4369e-06,  7.5091e-06,  3.0264e-01,\n",
       "       -9.6379e-08,  8.1397e-02,  9.2136e-04,  3.6383e-02,  2.9526e-01,\n",
       "       -3.5428e-06,  4.7736e-06,  1.0299e-05,  8.9118e-07,  7.4479e-06])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def g(x):\n",
    "    return -f(x)\n",
    "cons = ({'type': 'eq', 'fun': lambda x: x.sum() - 1},\n",
    "        {'type': 'ineq', 'fun': lambda x: x.min()})\n",
    "res = minimize(g, x0, constraints = cons)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
