{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sps\n",
    "import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_proj_simplex(v, s=1):\n",
    "    n, = v.shape  # will raise ValueError if v is not 1-D\n",
    "    # check if we are already on the simplex\n",
    "    if v.sum() == s and np.alltrue(v >= 0):\n",
    "        # best projection: itself!\n",
    "        return v\n",
    "    # get the array of cumulative sums of a sorted (decreasing) copy of v\n",
    "    u = np.sort(v)[::-1]\n",
    "    cssv = np.cumsum(u)\n",
    "    # get the number of > 0 components of the optimal solution\n",
    "    rho = np.nonzero(u * np.arange(1, n+1) > (cssv - s))[0][-1]\n",
    "    # compute the Lagrange multiplier associated to the simplex constraint\n",
    "    theta = (cssv[rho] - s) / (rho + 1.0)\n",
    "    # compute the projection by thresholding v using theta\n",
    "    w = (v - theta).clip(min=0)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent method\n",
    "\n",
    "def grad_descent(x0, alpha, precision, max_iters):\n",
    "    cur_x = x0\n",
    "    previous_step_size = 1\n",
    "    iters = 0\n",
    "    \n",
    "    while previous_step_size > precision and iters < 1000:\n",
    "        prev_x = cur_x\n",
    "        cur_x = np.abs(prev_x + alpha * df(prev_x))\n",
    "        print(f(cur_x))\n",
    "        previous_step_size = np.linalg.norm(cur_x - prev_x)\n",
    "        iters += 1\n",
    "        \n",
    "    print(\"The local maximum of proj grad occurs at\", cur_x)\n",
    "    print(\"Number of iterations: \", iters)\n",
    "    return cur_x\n",
    "\n",
    "def ternary_search(func, left, right, precision):\n",
    "    if abs(right - left) < precision:\n",
    "        return (left + right) / 2\n",
    "\n",
    "    left_third = (2*left + right)/3\n",
    "    right_third = (left + 2*right)/3\n",
    "\n",
    "    if func(left_third) < func(right_third):\n",
    "        return ternary_search(func, left_third, right, precision) \n",
    "    else:\n",
    "        return ternary_search(func, left, right_third, precision)\n",
    "\n",
    "def get_step_size(x, grad, precision):\n",
    "    zvalue = f(x)\n",
    "    def h(alpha):\n",
    "        return f(euclidean_proj_simplex(x + alpha * grad))\n",
    "    \n",
    "    return ternary_search(h, 0, 1000000, precision)\n",
    "    \n",
    "def proj_grad_descent(x0, alpha, precision, max_iters):\n",
    "    cur_x = x0\n",
    "    previous_step_size = 1\n",
    "    iters = 0\n",
    "    while previous_step_size > precision and iters < max_iters:\n",
    "        prev_x = cur_x\n",
    "        alpha = get_step_size(cur_x, df(cur_x), precision)\n",
    "        print(alpha, f(cur_x), cur_x)\n",
    "        cur_x = euclidean_proj_simplex(prev_x + alpha * df(prev_x))\n",
    "        \n",
    "        previous_step_size = np.linalg.norm(cur_x - prev_x)\n",
    "        iters += 1\n",
    "        \n",
    "    print(\"The local maximum of proj grad occurs at\", cur_x)\n",
    "    print(\"Number of iterations: \", iters)\n",
    "    return cur_x\n",
    "\n",
    "# Quasi-Newton method\n",
    "\n",
    "def proj_quasi_newton_method (x0, alpha, precision, max_iters):\n",
    "    cur_x = x0\n",
    "    previous_step_size = 1\n",
    "    iters = 0\n",
    "    H = np.eye(np.size(x0))\n",
    "    while previous_step_size > precision and iters < max_iters:\n",
    "        prev_x = cur_x\n",
    "        h = -H @ df(prev_x)\n",
    "        alpha = get_step_size(cur_x, df(cur_x), precision)\n",
    "        print(alpha, cur_x, f(cur_x))\n",
    "        cur_x = euclidean_proj_simplex(prev_x + alpha * h)\n",
    "        s = np.atleast_2d(cur_x - prev_x).T\n",
    "        y = np.atleast_2d(df(cur_x) - df(prev_x)).T\n",
    "        #print(s, y)\n",
    "        H = H - (H @ y @ y.T @ H) / (y.T @ H @ y) + (s @ s.T) / (y.T @ s)\n",
    "        previous_step_size = np.linalg.norm(cur_x - prev_x) \n",
    "        iters += 1\n",
    "    \n",
    "    print(\"The local maximum of quasi newton occurs at\", cur_x)\n",
    "    print(\"Number of iterations: \", iters)\n",
    "    return cur_x\n",
    "\n",
    "# Newton method \n",
    "\n",
    "def proj_newton_method (x0, alpha, precision, max_iters):\n",
    "    cur_x = x0\n",
    "    previous_step_size = 1\n",
    "    iters = 0\n",
    "    while previous_step_size > precision and iters < max_iters:\n",
    "        prev_x = cur_x\n",
    "        h =  - np.linalg.inv(ddf(prev_x)) @ df(prev_x)\n",
    "        alpha = get_step_size(cur_x, h, precision)\n",
    "        cur_x = euclidean_proj_simplex(prev_x + alpha * h)\n",
    "        previous_step_size = np.linalg.norm(cur_x - prev_x) \n",
    "        iters += 1\n",
    "    \n",
    "    print(\"The local maximum of newton occurs at\", cur_x)\n",
    "    print(\"Number of iterations: \", iters)\n",
    "    return cur_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_matrix = np.random.rand(5,5) \n",
    "t_matrix /= t_matrix.sum(axis=1)[:,None]\n",
    "\n",
    "c_matrix = (t_matrix * np.log2(t_matrix)).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = lambda x: c_matrix.T @ x - ((t_matrix @ x) * np.log2(t_matrix @ x)).sum(axis=-1)\n",
    "\n",
    "#df = lambda x: c_matrix.T - (t_matrix.T * (np.log2(t_matrix @ x) + 1)).sum(axis=-1)\n",
    "\n",
    "def f(x):\n",
    "    array = (t_matrix @ x) * np.log2(t_matrix @ x)\n",
    "    return c_matrix @ x - array.sum() \n",
    "\n",
    "def df(x):\n",
    "    k = t_matrix.shape[0]\n",
    "    array = np.array([t_matrix[i] * (np.log2(t_matrix[i] @ x) + 1) for i in range(k)])\n",
    "    return c_matrix - array.sum(axis=0)\n",
    "\n",
    "def ddf(x):\n",
    "    k = t_matrix.shape[0]\n",
    "    a = t_matrix @ x\n",
    "    array = np.array([np.atleast_2d(t_matrix[i]).T @ np.atleast_2d(t_matrix[i]) for i in range(k)])\n",
    "    result = - np.array([a[i] * array[i] for i in range(k)]).sum(axis=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.183889024768779 0.1878149646467988 [0.17246696 0.19326059 0.19246463 0.24736365 0.19444416]\n",
      "0.6447574154632922 0.2529902145555407 [0.43731771 0.38229515 0.17432424 0.0060629  0.        ]\n",
      "0.30236196354157285 0.2549692344938801 [0.41309623 0.3351782  0.22032477 0.03140079 0.        ]\n",
      "0.20033908909249531 0.2550786357402699 [0.41028645 0.32130277 0.23240641 0.03600437 0.        ]\n",
      "0.16814950140508084 0.25510872655663186 [0.40931219 0.31362248 0.23915097 0.03791436 0.        ]\n",
      "0.1721976011535587 0.2551246393204343 [0.40883667 0.30786091 0.24427793 0.03902449 0.        ]\n",
      "0.21640982156986088 0.25513786685208784 [0.40858404 0.30247722 0.24913882 0.03979993 0.        ]\n",
      "0.3528849591204743 0.2551544087627211 [0.40851655 0.29630842 0.25480831 0.04036672 0.        ]\n",
      "0.8920151498878677 0.255187287393988 [0.40882873 0.28734643 0.26326366 0.04056118 0.        ]\n",
      "5.087857253207961 0.25531736694305884 [0.41097124 0.26863043 0.28187341 0.03852492 0.        ]\n",
      "9.535865027967928 0.2576602925310567 [4.35548266e-01 2.06841615e-01 3.57610102e-01 1.68266188e-08\n",
      " 0.00000000e+00]\n",
      "4.521886341908315e-06 0.2593509429960954 [0.43234488 0.27187643 0.29577869 0.         0.        ]\n",
      "The local maximum of proj grad occurs at [4.32344863e-01 2.71876346e-01 2.95778784e-01 7.44924975e-09\n",
      " 0.00000000e+00]\n",
      "Number of iterations:  12\n",
      "max proj grad capacity: 0.25935094208101184\n",
      "The local maximum of newton occurs at [0. 0. 0. 1. 0.]\n",
      "Number of iterations:  1511\n",
      "max proj newton capacity: 0.0\n",
      "The local maximum of quasi newton occurs at [0. 1. 0. 0. 0.]\n",
      "Number of iterations:  354\n",
      "max proj quasi newton capacity: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "x0 = np.random.rand(5)\n",
    "x0 /= x0.sum()\n",
    "alpha = 0.01\n",
    "precision = 10**-5\n",
    "iters = 10**5\n",
    "\n",
    "x1 = proj_grad_descent(x0, alpha, precision, iters)\n",
    "print(\"max proj grad capacity:\", f(x1))\n",
    "x2 = proj_newton_method(x0, alpha, precision, iters)\n",
    "print(\"max proj newton capacity:\", f(x2))\n",
    "x3 = proj_quasi_newton_method(x0, alpha, precision, iters)\n",
    "print(\"max proj quasi newton capacity:\", f(x3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_matrix = np.matrix([[0.9, 0.1], [0.1, 0.9]])\n",
    "t_matrix = np.array(t_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27807190511263774\n"
     ]
    }
   ],
   "source": [
    "print(1 + 0.2*np.log2(0.2) + 0.8*np.log2(0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = (t_matrix * np.log2(t_matrix)).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45 0.55]\n",
      "[-0.37183391 -0.55689435]\n",
      "0.5403639815795979 [0.45 0.55] 0.5263828452309445\n",
      "31.47103936854503 [0.4 0.6] 0.5124583024673183\n",
      "0.3943310945706878 [1. 0.] 0.0\n",
      "The local maximum of quasi newton occurs at [1. 0.]\n",
      "Number of iterations:  3\n",
      "max proj quasi grad cap: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([0.45, 0.55])\n",
    "x0 /= x0.sum()\n",
    "print(x0)\n",
    "alpha = 0.1\n",
    "precision = 10**-8\n",
    "iters = 10**5\n",
    "print(df(x0))\n",
    "\n",
    "x3 = proj_quasi_newton_method(x0, alpha, precision, iters)\n",
    "print(\"max proj quasi grad cap:\", f(x3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
