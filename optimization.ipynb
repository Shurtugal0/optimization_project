{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sps\n",
    "import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_proj_simplex(v, s=1):\n",
    "    n, = v.shape  # will raise ValueError if v is not 1-D\n",
    "    # check if we are already on the simplex\n",
    "    if v.sum() == s and np.alltrue(v >= 0):\n",
    "        # best projection: itself!\n",
    "        return v\n",
    "    # get the array of cumulative sums of a sorted (decreasing) copy of v\n",
    "    u = np.sort(v)[::-1]\n",
    "    cssv = np.cumsum(u)\n",
    "    # get the number of > 0 components of the optimal solution\n",
    "    rho = np.nonzero(u * np.arange(1, n+1) > (cssv - s))[0][-1]\n",
    "    # compute the Lagrange multiplier associated to the simplex constraint\n",
    "    theta = (cssv[rho] - s) / (rho + 1.0)\n",
    "    # compute the projection by thresholding v using theta\n",
    "    w = (v - theta).clip(min=0)\n",
    "    return w\n",
    "\n",
    "\n",
    "def ternary_search(func, left, right, precision):\n",
    "    # finds argmax of convex function f\n",
    "    # on the segment [left; right] with given precision\n",
    "    if abs(right - left) < precision:\n",
    "        return left\n",
    "\n",
    "    left_third = (2*left + right)/3\n",
    "    right_third = (left + 2*right)/3\n",
    "    if func(left_third) == func(right_third):\n",
    "        return ternary_search(func, left_third, right_third, precision) \n",
    "    if func(left_third) < func(right_third):\n",
    "        return ternary_search(func, left_third, right, precision) \n",
    "    else:\n",
    "        return ternary_search(func, left, right_third, precision)\n",
    "\n",
    "\n",
    "def search(f, estimate, precision):\n",
    "    # finds argmax of convex function f\n",
    "    # on the line with given precision\n",
    "    curr = estimate\n",
    "    delta = 1\n",
    "    while f(curr + delta) > f(curr):\n",
    "        curr = curr + delta\n",
    "        delta *= 2\n",
    "    curr_left = estimate\n",
    "    delta_left = 1\n",
    "    while f(curr_left - delta_left) > f(curr_left):\n",
    "        curr_left = curr_left - delta_left\n",
    "        delta_left *= 2\n",
    "    return ternary_search(f, curr_left - delta_left, curr + delta, precision)\n",
    "    \n",
    "    \n",
    "def get_step_size(x, grad, precision):\n",
    "    # steepest descent\n",
    "    def h(alpha):\n",
    "        return f(euclidean_proj_simplex(x + alpha * grad))\n",
    "    optimal_alpha = search(h, 0, precision)\n",
    "    return optimal_alpha\n",
    "\n",
    "def get_constant_step_size(alpha):\n",
    "    def get_size(x, grad, precision):\n",
    "        return alpha\n",
    "    return get_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent method\n",
    "\n",
    "def proj_grad_descent(x0, alpha_generator, precision, max_iters):\n",
    "    cur_x = x0\n",
    "    previous_step_size = 1\n",
    "    iters = 0\n",
    "    while previous_step_size > precision and iters < max_iters:\n",
    "        prev_x = cur_x\n",
    "        alpha = alpha_generator(cur_x, df(cur_x), 1e-7)\n",
    "        cur_x = euclidean_proj_simplex(prev_x + alpha * df(prev_x))\n",
    "        previous_step_size = np.linalg.norm(cur_x - prev_x)\n",
    "        iters += 1\n",
    "        \n",
    "    np.set_printoptions(precision=4)\n",
    "    print(\"The local maximum of proj grad occurs at\", cur_x)\n",
    "    print(\"Number of iterations: \", iters)\n",
    "    return cur_x\n",
    "\n",
    "# Quasi-Newton method\n",
    "\n",
    "def proj_quasi_newton_method (x0, alpha, precision, max_iters):\n",
    "    cur_x = x0\n",
    "    previous_step_size = 1\n",
    "    iters = 0\n",
    "    H = np.eye(np.size(x0))\n",
    "    alpha = 1\n",
    "    while previous_step_size > precision and iters < max_iters:\n",
    "        prev_x = cur_x\n",
    "        h = -H @ df(prev_x)\n",
    "        alpha = get_step_size(prev_x, h, 1e-7)\n",
    "        cur_x = euclidean_proj_simplex(prev_x + alpha * h)\n",
    "        s = np.atleast_2d(cur_x - prev_x).T\n",
    "        y = np.atleast_2d(df(cur_x) - df(prev_x)).T\n",
    "        #print(s, y)\n",
    "        H = H - (H @ y @ y.T @ H) / (y.T @ H @ y) + (s @ s.T) / (y.T @ s)\n",
    "        previous_step_size = np.linalg.norm(cur_x - prev_x) \n",
    "        iters += 1\n",
    "        \n",
    "    np.set_printoptions(precision=4)\n",
    "    print(\"The local maximum of quasi newton occurs at\", cur_x)\n",
    "    print(\"Number of iterations: \", iters)\n",
    "    return cur_x\n",
    "\n",
    "# Newton method \n",
    "\n",
    "def proj_newton_method (x0, alpha, precision, max_iters):\n",
    "    cur_x = x0\n",
    "    previous_step_size = 1\n",
    "    iters = 0\n",
    "    while previous_step_size > precision and iters < max_iters:\n",
    "        prev_x = cur_x\n",
    "        h = - np.linalg.inv(ddf(prev_x)) @ df(prev_x)\n",
    "        alpha = get_step_size(prev_x, h, 1e-7)\n",
    "      \n",
    "        cur_x = euclidean_proj_simplex(prev_x + alpha * h)\n",
    "        previous_step_size = np.linalg.norm(cur_x - prev_x) \n",
    "        iters += 1\n",
    "        \n",
    "    np.set_printoptions(precision=4)\n",
    "    print(\"The local maximum of newton occurs at\", cur_x)\n",
    "    print(\"Number of iterations: \", iters)\n",
    "    return cur_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_matrix = np.random.rand(5,5) \n",
    "t_matrix /= t_matrix.sum(axis=1)[:,None]\n",
    "t_matrix = t_matrix.T\n",
    "c_matrix = (t_matrix * np.log2(t_matrix)).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    array = (t_matrix @ x) * np.log2(t_matrix @ x)\n",
    "    return c_matrix @ x - array.sum() \n",
    "\n",
    "def df(x):\n",
    "    k = t_matrix.shape[0]\n",
    "    array = np.array([t_matrix[i] * (np.log2(t_matrix[i] @ x) + 1) for i in range(k)])\n",
    "    return c_matrix - array.sum(axis=0)\n",
    "\n",
    "def ddf(x):\n",
    "    k = t_matrix.shape[0]\n",
    "    a = t_matrix @ x\n",
    "    array = np.array([np.atleast_2d(t_matrix[i]).T @ np.atleast_2d(t_matrix[i]) for i in range(k)])\n",
    "    result = - np.array([array[i] / a[i] for i in range(k)]).sum(axis=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: -0.21136588462777306\n",
      "     jac: array([1.2494, 1.2313, 1.2313, 1.2535, 1.2313])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 117\n",
      "     nit: 15\n",
      "    njev: 15\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-7.0571e-09,  1.0771e-01,  4.2527e-01, -1.3431e-17,  4.6702e-01])\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def g(x):\n",
    "    return -f(x)\n",
    "cons = ({'type': 'eq', 'fun': lambda x: x.sum() - 1},\n",
    "        {'type': 'ineq', 'fun': lambda x: x.min()})\n",
    "res = minimize(g, x0, constraints = cons)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting point: [0.3803 0.2099 0.0227 0.1901 0.1971]\n",
      "The local maximum of proj grad occurs at [0.     0.1078 0.4252 0.     0.467 ]\n",
      "Number of iterations:  13\n",
      "max proj grad capacity: 0.21136588684797175\n",
      "The local maximum of proj grad occurs at [0.     0.1078 0.4252 0.     0.467 ]\n",
      "Number of iterations:  242\n",
      "max proj grad capacity with constant step: 0.21136588672637124\n",
      "The local maximum of quasi newton occurs at [7.6922e-02 4.3687e-02 4.7115e-01 2.2407e-08 4.0824e-01]\n",
      "Number of iterations:  4\n",
      "max proj quasi newton capacity: 0.20770512307345346\n",
      "The local maximum of newton occurs at [0.0046 0.101  0.4526 0.0039 0.4379]\n",
      "Number of iterations:  4\n",
      "max proj newton capacity: 0.2106701549819734\n"
     ]
    }
   ],
   "source": [
    "x0 = np.random.rand(5)\n",
    "x0 /= x0.sum()\n",
    "precision = 10**-6\n",
    "iters = 10**4\n",
    "\n",
    "print('Starting point:', x0)\n",
    "\n",
    "x1 = proj_grad_descent(x0, get_step_size, precision, iters)\n",
    "print(\"max proj grad capacity:\", f(x1))\n",
    "x2 = proj_grad_descent(x0, get_constant_step_size(0.1), precision, iters)\n",
    "print(\"max proj grad capacity with constant step:\", f(x2))\n",
    "x3 = proj_quasi_newton_method (x0, get_step_size, precision * 10, iters)\n",
    "print(\"max proj quasi newton capacity:\", f(x3))\n",
    "x4 = proj_newton_method (x0, get_step_size, precision, iters)\n",
    "print(\"max proj newton capacity:\", f(x4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_matrix = np.matrix([[0.9, 0.1], [0.1, 0.9]])\n",
    "t_matrix = np.array(t_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5310044064107187\n"
     ]
    }
   ],
   "source": [
    "print(1 + 0.1*np.log2(0.1) + 0.9*np.log2(0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = (t_matrix * np.log2(t_matrix)).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3497 0.6503]\n",
      "[-0.1429 -0.7091]\n",
      "The local maximum of proj grad occurs at [0.5 0.5]\n",
      "Number of iterations:  3\n",
      "max proj grad capacity: 0.5310044064107188\n",
      "The local maximum of proj grad occurs at [0.5 0.5]\n",
      "Number of iterations:  76\n",
      "max proj grad capacity with constant step: 0.5310044064107174\n",
      "The local maximum of quasi newton occurs at [0.5 0.5]\n",
      "Number of iterations:  2\n",
      "max proj quasi newton capacity: 0.5310044064107187\n",
      "The local maximum of newton occurs at [0.5 0.5]\n",
      "Number of iterations:  2\n",
      "max proj newton capacity: 0.5310044064107184\n"
     ]
    }
   ],
   "source": [
    "x0 = np.random.rand(2)\n",
    "x0 /= x0.sum()\n",
    "print(x0)\n",
    "alpha = 0.1\n",
    "precision = 10**-8\n",
    "iters = 10**5\n",
    "print(df(x0))\n",
    "\n",
    "x1 = proj_grad_descent(x0, get_step_size, precision, iters)\n",
    "print(\"max proj grad capacity:\", f(x1))\n",
    "x2 = proj_grad_descent(x0, get_constant_step_size(0.1), precision, iters)\n",
    "print(\"max proj grad capacity with constant step:\", f(x2))\n",
    "x3 = proj_quasi_newton_method (x0, get_step_size, precision * 10, iters)\n",
    "print(\"max proj quasi newton capacity:\", f(x3))\n",
    "x4 = proj_newton_method (x0, get_step_size, precision, iters)\n",
    "print(\"max proj newton capacity:\", f(x4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: -0.5310044056284965\n",
      "     jac: array([0.9117, 0.9117])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 9\n",
      "     nit: 2\n",
      "    njev: 2\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.5, 0.5])\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def g(x):\n",
    "    return -f(x)\n",
    "cons = ({'type': 'eq', 'fun': lambda x: x.sum() - 1},\n",
    "        {'type': 'ineq', 'fun': lambda x: x.min()})\n",
    "res = minimize(g, x0, constraints = cons)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
